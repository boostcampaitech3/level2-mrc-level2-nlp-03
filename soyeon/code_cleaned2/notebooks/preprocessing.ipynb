{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bee1c4e-8db1-4be5-a8a5-71dc64c5c9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bec31cb-d29b-46f3-aef9-183b1b467258",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'klue/bert-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(name,\n",
    "                                          \n",
    "                                          use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fab18ff-14d1-43d6-8d77-a490ab3537c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(name,\n",
    "                                          max_len=10,\n",
    "                                          use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a61b9603-07a2-455d-981f-6f62586b63dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(sentence[0],max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04589bab-b849-4cc7-b130-720821af42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = ['이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "               이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다. \\\n",
    "                이 생겼을 때, 아주 당연하게 검색엔진을 활용하여 검색을 합니다. 이런 검색엔진은 최근 MRC (기계독해) 기술을 활용하며 매일 발전하고 있는데요. 본 대회에서는 우리가 당연하게 활용하던 검색엔진, \\\n",
    "                그것과 유사한 형태의 시스템을 만들어 볼 것입니다.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc8ce083-4601-47a6-82a7-8a9a6982c08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 575, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 1504, 5810, 2069, 904, 16, 4129, 4685, 2205, 2318, 6102, 2614, 2043, 2069, 3987, 7488, 6102, 2069, 3803, 18, 3667, 6102, 2614, 2043, 2073, 3744, 20606, 2108, 12, 5276, 2405, 2097, 13, 3726, 2069, 3987, 2205, 2307, 4709, 3859, 19521, 1513, 13964, 2182, 18, 1163, 3931, 27135, 2259, 3616, 2116, 4685, 2205, 2318, 3987, 2205, 2414, 6102, 2614, 2043, 16, 3724, 2145, 5670, 2470, 4337, 2079, 4119, 2069, 3679, 2051, 1164, 575, 12190, 18, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sentence,\n",
    "         #model_max_len=10, # model_max_len이랑 뭐가 다름\n",
    "         stride=0,\n",
    "         return_overflowing_tokens=False,\n",
    "         return_offsets_mapping=False,\n",
    "         padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b6cb482a-2f82-491b-86ee-cf949a4151d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "682"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tokenizer(sentence,\n",
    "         #model_max_len=10, # model_max_len이랑 뭐가 다름\n",
    "         stride=0,\n",
    "         return_overflowing_tokens=False,\n",
    "         return_offsets_mapping=False,\n",
    "         padding='max_length')\n",
    "len(output['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "90462a9a-9d4c-4ac6-86cb-3a751ef9296d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['이', '생겼', '##을', '때', ',', '아주', '당연', '##하', '##게', '검색']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(sentence[0],\n",
    "                       max_length=10, \n",
    "                       truncation=True,\n",
    "                        return_overflowing_tokens=True,\n",
    "                  return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b66924d8-7f2a-4fdc-bf4f-891b4241f6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 3], [2, 4100, 2119, 4093, 5803, 2119, 4093, 1469, 2168, 3], [2, 2428, 1131, 2116, 5032, 2062, 1958, 2376, 3], [2, 4647, 2234, 4177, 2231, 8969, 3926, 2918, 3683, 3], [2, 1, 6741, 918, 4405, 2321, 1468, 2182, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (7, 8), (8, 9), (10, 12), (0, 0)], [(0, 0), (13, 15), (15, 16), (17, 19), (20, 22), (22, 23), (24, 26), (27, 28), (28, 29), (0, 0)], [(0, 0), (29, 30), (31, 32), (32, 33), (34, 36), (36, 37), (38, 39), (39, 40), (0, 0)], [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (7, 9), (10, 12), (12, 13), (13, 15), (0, 0)], [(0, 0), (16, 27), (28, 32), (33, 34), (35, 37), (37, 38), (39, 40), (40, 41), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence=['배가아푸당당 배가 아파 머리도 아파 허리도 아파 우오옷 배가 아프다 흐악',\n",
    "             '이거슨 무슨말인지 모르겠지만 롤롤롤롱힝히잏ㅇ힝말은 그렇지만 또 다르죠 용요']\n",
    "tokenizer(new_sentence,\n",
    "          max_length=10, \n",
    "           truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "66d142e0-a9a2-4018-8377-af736ea2f2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 3], [2, 29483, 1131, 2116, 4093, 4100, 2119, 4093, 5803, 3], [2, 4100, 2119, 4093, 5803, 2119, 4093, 1469, 2168, 3], [2, 2119, 4093, 1469, 2168, 2428, 1131, 2116, 5032, 3], [2, 2428, 1131, 2116, 5032, 2062, 1958, 2376, 3], [2, 4647, 2234, 4177, 2231, 8969, 3926, 2918, 3683, 3], [2, 8969, 3926, 2918, 3683, 1, 6741, 918, 4405, 3], [2, 1, 6741, 918, 4405, 2321, 1468, 2182, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 6), (7, 8), (8, 9), (10, 12), (0, 0)], [(0, 0), (4, 6), (7, 8), (8, 9), (10, 12), (13, 15), (15, 16), (17, 19), (20, 22), (0, 0)], [(0, 0), (13, 15), (15, 16), (17, 19), (20, 22), (22, 23), (24, 26), (27, 28), (28, 29), (0, 0)], [(0, 0), (22, 23), (24, 26), (27, 28), (28, 29), (29, 30), (31, 32), (32, 33), (34, 36), (0, 0)], [(0, 0), (29, 30), (31, 32), (32, 33), (34, 36), (36, 37), (38, 39), (39, 40), (0, 0)], [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (7, 9), (10, 12), (12, 13), (13, 15), (0, 0)], [(0, 0), (7, 9), (10, 12), (12, 13), (13, 15), (16, 27), (28, 32), (33, 34), (35, 37), (0, 0)], [(0, 0), (16, 27), (28, 32), (33, 34), (35, 37), (37, 38), (39, 40), (40, 41), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 1, 1, 1]}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "new_sentence=['배가아푸당당 배가 아파 머리도 아파 허리도 아파 우오옷 배가 아프다 흐악',\n",
    "             '이거슨 무슨말인지 모르겠지만 롤롤롤롱힝히잏ㅇ힝말은 그렇지만 또 다르죠 용요']\n",
    "out = tokenizer(new_sentence,\n",
    "          max_length=10, \n",
    "           truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "         stride=4)\n",
    "print(out)\n",
    "print(len(out['offset_mapping']) == len(out['overflow_to_sample_mapping']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f210107-1254-41ed-9a80-e7f5fc7fb793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1131, 2116, 2227, 3], [2, 2257, 29483, 1131, 3], [2, 2116, 4093, 4100, 3], [2, 2119, 4093, 5803, 3], [2, 2119, 4093, 1469, 3], [2, 2168, 2428, 1131, 3], [2, 2116, 5032, 2062, 3], [2, 1958, 2376, 3]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 1), (1, 2), (2, 3), (0, 0)], [(0, 0), (3, 4), (4, 6), (7, 8), (0, 0)], [(0, 0), (8, 9), (10, 12), (13, 15), (0, 0)], [(0, 0), (15, 16), (17, 19), (20, 22), (0, 0)], [(0, 0), (22, 23), (24, 26), (27, 28), (0, 0)], [(0, 0), (28, 29), (29, 30), (31, 32), (0, 0)], [(0, 0), (32, 33), (34, 36), (36, 37), (0, 0)], [(0, 0), (38, 39), (39, 40), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence=['배가아푸당당 배가 아파 머리도 아파 허리도 아파 우오옷 배가 아프다 흐악']\n",
    "tokenizer(new_sentence,\n",
    "          max_length=5, \n",
    "           truncation=True,\n",
    "          return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c2efa4fb-ad1c-4eb5-9f74-23012ab56b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 4100]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(new_sentence[0])[:10] # 2는 [CLS], 3은 [SEP] // # 왜 384 쓴다했는지 기억나나??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6ca77db2-027e-4fe6-a25e-dc1490d8d916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['배', '##가', '##아', '##푸', '##당당', '배', '##가', '아파', '머리', '##도']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(new_sentence[0])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0d38fcc5-f7e3-4462-9d7a-c6a26bb05e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3890, 2069, 1041, 19521, 1335, 2279, 2762, 6641, 3]\n",
      "['무엇', '##을', '말', '##하고', '싶', '##으', '##셈', '도대체']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sents = ['무엇을 말하고 싶으셈 도대체']\n",
    "print(tokenizer.encode(test_sents[0]))\n",
    "print(tokenizer.tokenize(test_sents[0]))\n",
    "(len(tokenizer.encode(test_sents[0])) -2) == len(tokenizer.tokenize(test_sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bed192a2-2c65-4430-b439-0f0362e89704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 3], [2, 4647, 2234, 4177, 2231, 8969, 3926, 2918, 3683, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence=['배가아푸당당 배가 아파 머리도 아파 허리도 아파 우오옷 배가 아프다 흐악',\n",
    "             '이거슨 무슨말인지 모르겠지만 롤롤롤롱힝히잏ㅇ힝말은 그렇지만 또 다르죠 용요']\n",
    "tokenizer(new_sentence,\n",
    "          max_length=10, \n",
    "           truncation=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "666e427d-63b8-40cd-b77a-c4dd7b2c8704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 3], [2, 4647, 2234, 4177, 2231, 8969, 3926, 2918, 3683, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(new_sentence,\n",
    "          max_length=10, stride=5,\n",
    "           truncation=True,\n",
    "         ) # return_overflowing_tokens=True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "beba2810-b25f-48c9-ab92-5707e6053cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 1131, 2116, 4093, 4100, 2119, 4093, 3], [2, 4093, 5803, 2119, 4093, 1469, 2168, 2428, 1131, 2116, 5032, 2062, 3], [2, 2062, 1958, 2376, 3], [2, 4647, 2234, 4177, 2231, 8969, 3926, 2918, 3683, 1, 6741, 918, 3], [2, 918, 4405, 2321, 1468, 2182, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(new_sentence,\n",
    "          max_length=13, stride=1,\n",
    "           truncation=True,return_overflowing_tokens=True,\n",
    "         ) # padding이 돼야하겠지.. 이거 padding안되고 그냥 학습에 들어가도 됨?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "42d0b9d9-11d8-4855-92d4-899ff46efc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = tokenizer(new_sentence,new_sentence,\n",
    "          max_length=13, stride=1,\n",
    "           truncation=True,return_overflowing_tokens=True,padding='max_length',\n",
    "         ) # padding이 돼야하겠지.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c048bff4-eeac-4b46-8c4f-fd0b195912f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, None]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.sequence_ids(0) # [2, 1131, 2116, 2227, 2257, 29483, 3, 1131, 2116, 2227, 2257, 29483, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "8424be49-c4c6-4198-94d5-c337d7b54ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, None]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.sequence_ids(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f200603-a52c-4b48-8afe-4518da4dc1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.sequence_ids of {'input_ids': [[2, 1131, 2116, 2227, 2257, 29483, 3, 1131, 2116, 2227, 2257, 29483, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 1131, 2116, 2227, 2257, 29483, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 29483, 1131, 2116, 4093, 4100, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 4100, 2119, 4093, 5803, 2119, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 2119, 4093, 1469, 2168, 2428, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 2428, 1131, 2116, 5032, 2062, 3], [2, 29483, 1131, 2116, 4093, 4100, 3, 2062, 1958, 2376, 3, 0, 0], [2, 4100, 2119, 4093, 5803, 2119, 3, 1131, 2116, 2227, 2257, 29483, 3], [2, 4100, 2119, 4093, 5803, 2119, 3, 29483, 1131, 2116, 4093, 4100, 3], [2, 4100, 2119, 4093, 5803, 2119, 3, 4100, 2119, 4093, 5803, 2119, 3], [2, 4100, 2119, 4093, 5803, 2119, 3, 2119, 4093, 1469, 2168, 2428, 3], [2, 4100, 2119, 4093, 5803, 2119, 3, 2428, 1131, 2116, 5032, 2062, 3], [2, 4100, 2119, 4093, 5803, 2119, 3, 2062, 1958, 2376, 3, 0, 0], [2, 2119, 4093, 1469, 2168, 2428, 3, 1131, 2116, 2227, 2257, 29483, 3], [2, 2119, 4093, 1469, 2168, 2428, 3, 29483, 1131, 2116, 4093, 4100, 3], [2, 2119, 4093, 1469, 2168, 2428, 3, 4100, 2119, 4093, 5803, 2119, 3], [2, 2119, 4093, 1469, 2168, 2428, 3, 2119, 4093, 1469, 2168, 2428, 3], [2, 2119, 4093, 1469, 2168, 2428, 3, 2428, 1131, 2116, 5032, 2062, 3], [2, 2119, 4093, 1469, 2168, 2428, 3, 2062, 1958, 2376, 3, 0, 0], [2, 2428, 1131, 2116, 5032, 2062, 3, 1131, 2116, 2227, 2257, 29483, 3], [2, 2428, 1131, 2116, 5032, 2062, 3, 29483, 1131, 2116, 4093, 4100, 3], [2, 2428, 1131, 2116, 5032, 2062, 3, 4100, 2119, 4093, 5803, 2119, 3], [2, 2428, 1131, 2116, 5032, 2062, 3, 2119, 4093, 1469, 2168, 2428, 3], [2, 2428, 1131, 2116, 5032, 2062, 3, 2428, 1131, 2116, 5032, 2062, 3], [2, 2428, 1131, 2116, 5032, 2062, 3, 2062, 1958, 2376, 3, 0, 0], [2, 2062, 1958, 2376, 3, 1131, 2116, 2227, 2257, 29483, 3, 0, 0], [2, 2062, 1958, 2376, 3, 29483, 1131, 2116, 4093, 4100, 3, 0, 0], [2, 2062, 1958, 2376, 3, 4100, 2119, 4093, 5803, 2119, 3, 0, 0], [2, 2062, 1958, 2376, 3, 2119, 4093, 1469, 2168, 2428, 3, 0, 0], [2, 2062, 1958, 2376, 3, 2428, 1131, 2116, 5032, 2062, 3, 0, 0], [2, 2062, 1958, 2376, 3, 2062, 1958, 2376, 3, 0, 0, 0, 0], [2, 1131, 2116, 2227, 2257, 29483, 3, 29483, 1131, 2116, 4093, 4100, 3], [2, 1131, 2116, 2227, 2257, 29483, 3, 4100, 2119, 4093, 5803, 2119, 3], [2, 1131, 2116, 2227, 2257, 29483, 3, 2119, 4093, 1469, 2168, 2428, 3], [2, 1131, 2116, 2227, 2257, 29483, 3, 2428, 1131, 2116, 5032, 2062, 3], [2, 1131, 2116, 2227, 2257, 29483, 3, 2062, 1958, 2376, 3, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.sequence_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1008fb-80ee-422d-9e1b-6c10dfb847a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82a9272e-3a2a-4b0d-bb06-ee3912fbeeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 15519, 809, 942, 1036, 1118, 1233, 1376, 1517, 3], [2, 1036, 1118, 1233, 1376, 1517, 1632, 1697, 1761, 3], [2, 1376, 1517, 1632, 1697, 1761, 1826, 1889, 3, 0], [2, 3651, 2718, 3649, 2553, 2315, 1, 189, 2755, 3], [2, 2553, 2315, 1, 189, 2755, 2044, 2060, 2030, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (0, 0)], [(0, 0), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (17, 18), (19, 20), (21, 22), (0, 0)], [(0, 0), (13, 14), (15, 16), (17, 18), (19, 20), (21, 22), (23, 24), (25, 26), (0, 0), (0, 0)], [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (7, 8), (9, 14), (15, 16), (16, 17), (0, 0)], [(0, 0), (6, 7), (7, 8), (9, 14), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 1, 1]}\n",
      "\n",
      "[[(0, 0), (0, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (0, 0)], [(0, 0), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16), (17, 18), (19, 20), (21, 22), (0, 0)], [(0, 0), (13, 14), (15, 16), (17, 18), (19, 20), (21, 22), (23, 24), (25, 26), (0, 0), (0, 0)], [(0, 0), (0, 2), (2, 3), (4, 6), (6, 7), (7, 8), (9, 14), (15, 16), (16, 17), (0, 0)], [(0, 0), (6, 7), (7, 8), (9, 14), (15, 16), (16, 17), (17, 18), (18, 19), (19, 20), (0, 0)]]\n"
     ]
    }
   ],
   "source": [
    "new_sentence=['가나 다 라 마 바 사 아 자 차 카 타 파 하',\n",
    "             '아이낭 그렇ㄷ래 니아ㅗ닝라 ㅇㄴ미ㅏ러']\n",
    "out = tokenizer(new_sentence,\n",
    "          max_length=10, stride=5,\n",
    "          truncation=True,return_overflowing_tokens=True,return_offsets_mapping=True,padding='max_length',\n",
    "         ) \n",
    "print(out)\n",
    "print()\n",
    "print(out['offset_mapping'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ff5c0d34-2d47-4375-8f1d-615ae4da2b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가나',\n",
       " '##다',\n",
       " '##라마',\n",
       " '##바',\n",
       " '##사',\n",
       " '##아',\n",
       " '##자',\n",
       " '##차',\n",
       " '##카',\n",
       " '##타',\n",
       " '##파',\n",
       " '##하']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(new_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c66fd8-599a-46d2-a9d2-ccd742859fbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8eb4d994-2547-42b4-8264-7d4fd8b6e066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 15519, 2062, 19045, 2100, 2063, 2227, 2155, 2232, 3], [2, 2100, 2063, 2227, 2155, 2232, 2127, 2256, 2160, 3], [2, 2155, 2232, 2127, 2256, 2160, 2205, 15519, 2062, 3], [2, 2256, 2160, 2205, 15519, 2062, 19045, 2100, 2063, 3], [2, 15519, 2062, 19045, 2100, 2063, 2227, 2155, 2232, 3], [2, 2100, 2063, 2227, 2155, 2232, 2127, 2256, 2160, 3], [2, 2155, 2232, 2127, 2256, 2160, 2205, 3, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sentence=['가나다라마바사아자차카타파하 가나다라마바사아자차카타파하']\n",
    "tokenizer(new_sentence,\n",
    "          max_length=10, stride=5,\n",
    "          truncation=True,return_overflowing_tokens=True,padding='max_length',\n",
    "         ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a992e20-80e6-4c67-bdc1-a03b5e58da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(new_sentence,\n",
    "        \n",
    "         ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eea7286-53f4-414b-ae00-e58ec718c958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "name = 'klue/roberta-large'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(name,\n",
    "                                          \n",
    "                                          use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01091300-4a90-483d-bbac-17f22363c87d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Truncation error: Second sequence not provided",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m new_sentence\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m가나 다 라 마 바 사 아 자 차 카 타 파 하\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m아이낭 그렇ㄷ래 니아ㅗ닝라 ㅇㄴ미ㅏ러\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m pad_on_right\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtokenizer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_sentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# truncation=True,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monly_second\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpad_on_right\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monly_first\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# stride=data_args.doc_stride,\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m    \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2455\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2451\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2452\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2453\u001b[0m         )\n\u001b[1;32m   2454\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2457\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2465\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2476\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2477\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2493\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2494\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2646\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2636\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2637\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2638\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2639\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2643\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2644\u001b[0m )\n\u001b[0;32m-> 2646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2663\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2664\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:425\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    418\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[1;32m    419\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    423\u001b[0m )\n\u001b[0;32m--> 425\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    437\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    439\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    449\u001b[0m ]\n",
      "\u001b[0;31mException\u001b[0m: Truncation error: Second sequence not provided"
     ]
    }
   ],
   "source": [
    "new_sentence=['가나 다 라 마 바 사 아 자 차 카 타 파 하',\n",
    "             '아이낭 그렇ㄷ래 니아ㅗ닝라 ㅇㄴ미ㅏ러']\n",
    "pad_on_right=True\n",
    "tokenizer2(new_sentence,\n",
    "    # truncation=True,\n",
    " truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=5,\n",
    "            # stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "            padding=\"max_length\"    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a461409-7d64-4f7d-9ff5-c390ba38f107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
